{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#проверка числа памяти, которая задейстована\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMTEws-L9g8P",
        "outputId": "39141ed6-99b5-45ad-a82c-2c5a65fff2d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar  5 09:38:59 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install  transformers==4.47.1 accelerate==1.2.1 sentencepiece==0.2.0 optimum==1.23.3 auto-gptq==0.7.1 torchmetrics\n",
        "%pip install -U bitsandbytes\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "import bitsandbytes as bnb\n",
        "import subprocess\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange\n",
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.functional import accuracy\n",
        "from torch.optim import AdamW\n",
        "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, LoraConfig, TaskType\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrmHAd6UB1N-",
        "outputId": "97bae81e-744a-4cf0-c4eb-5dab01991ba0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.47.1 in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: accelerate==1.2.1 in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: optimum==1.23.3 in /usr/local/lib/python3.11/dist-packages (1.23.3)\n",
            "Requirement already satisfied: auto-gptq==0.7.1 in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.1) (2.5.1+cu124)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from optimum==1.23.3) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from optimum==1.23.3) (1.13.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from optimum==1.23.3) (3.3.2)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (from auto-gptq==0.7.1) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.11/dist-packages (from auto-gptq==0.7.1) (1.2.1)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq==0.7.1) (0.14.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==1.2.1) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->optimum==1.23.3) (1.3.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->optimum==1.23.3) (10.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->optimum==1.23.3) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->optimum==1.23.3) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->optimum==1.23.3) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->optimum==1.23.3) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->optimum==1.23.3) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->optimum==1.23.3) (3.11.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (2025.1.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto-gptq==0.7.1) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->optimum==1.23.3) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==1.2.1) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->optimum==1.23.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->optimum==1.23.3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->optimum==1.23.3) (2025.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install sacrebleu\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "eFN8dLKDJ9kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddfa60f-c718-4d2c-d7a6-3e4d60bd2e6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ce32370b97746709b19a85c566d0e3dadd9ed4ef01bec59dee4fc7f9fba207ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.47.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import ast\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def parse_all_data(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Парсинг всех данных без учета времени ответа\"\"\"\n",
        "    return _parse_data(file_path, include_time=False)\n",
        "\n",
        "def parse_data_with_time(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Парсинг данных с сохранением времени ответа\"\"\"\n",
        "    return _parse_data(file_path, include_time=True)\n",
        "\n",
        "def _parse_data(file_path: str, include_time: bool) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Базовая функция парсинга\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    result = []\n",
        "    for item in data:\n",
        "        parsed = {\n",
        "            'selected_role': item['Выбранная роль'],\n",
        "            'campus': item['Кампус'],\n",
        "            'education_level': item['Уровень образования'],\n",
        "            'question_category': item['Категория вопроса'],\n",
        "            'user_question': _clean_text(item['Вопрос пользователя']),\n",
        "            'user_filters': item['user_filters'],\n",
        "            'question_filters': item['question_filters'],\n",
        "            'saiga_answer': _clean_text(item['Saiga']),\n",
        "            'giga_answer': _clean_text(item['Giga']),\n",
        "            'winner': item['Кто лучше?'],\n",
        "            'comment': item['Комментарий'],\n",
        "            'contexts': _parse_contexts(item['Ресурсы для ответа'])\n",
        "        }\n",
        "\n",
        "        if item.get('Уточненный вопрос пользователя'):\n",
        "            parsed.update({\n",
        "                'refined_question': _clean_text(item['Уточненный вопрос пользователя']),\n",
        "                'refined_answer': _clean_text(item['Ответ AI (уточнение)']),\n",
        "                'refined_contexts': _parse_contexts(item['Ресурсы для ответа (уточнение)'] or '')\n",
        "            })\n",
        "\n",
        "        if include_time:\n",
        "            parsed.update({\n",
        "                'response_time': item['Время ответа модели (сек)'],\n",
        "                'refined_response_time': item.get('Время ответа модели на уточненный вопрос (сек)')\n",
        "            })\n",
        "\n",
        "        result.append(parsed)\n",
        "\n",
        "    return result\n",
        "\n",
        "def _parse_contexts(resources: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Парсинг контекстов с использованием вашей функции\"\"\"\n",
        "    contexts = []\n",
        "    pattern = re.compile(r\"Document\\(page_content='(.*?)', metadata=({.*?})\\)\", re.DOTALL)\n",
        "\n",
        "    for match in re.finditer(pattern, resources):\n",
        "        content, metadata_str = match.groups()\n",
        "        try:\n",
        "            metadata = ast.literal_eval(metadata_str)\n",
        "            tags = _extract_tags(metadata)\n",
        "\n",
        "            contexts.append({\n",
        "                'text': _clean_text(content),\n",
        "                'metadata': {\n",
        "                    'source': metadata.get('source'),\n",
        "                    'file_name': metadata.get('file_name'),\n",
        "                    'url': metadata.get('url')\n",
        "                },\n",
        "                'tags': tags\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Контекст не распарсился: {e}\")\n",
        "\n",
        "    return contexts\n",
        "\n",
        "def _extract_tags(metadata: Dict) -> Dict[str, List[str]]:\n",
        "    \"\"\"Извлечение тегов в отдельные категории\"\"\"\n",
        "    return {\n",
        "        'topic_tags': [v for k,v in metadata.items() if k.startswith('topic_tag_') and v],\n",
        "        'user_tags': [v for k,v in metadata.items() if k.startswith('user_tag_') and v]\n",
        "    }\n",
        "\n",
        "def _clean_text(text: str) -> str:\n",
        "    \"\"\"Очистка текста\"\"\"\n",
        "    if not text: return ''\n",
        "    return re.sub(r'\\\\[nrt]|[\\n\\r\\t]+|\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "xpIg0ipwKJ3K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_v1 = parse_all_data('train_set.json')\n",
        "\n",
        "data_v2 = parse_all_data('val_set.json')\n",
        "\n",
        "with open('parsed_tuning.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(data_v1, f, ensure_ascii=False)\n",
        "\n",
        "with open('parsed_dash.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(data_v2, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "K3ubdCwEKNYh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('parsed_tuning.json', 'r', encoding='utf-8') as f:\n",
        "    training_data = json.load(f)"
      ],
      "metadata": {
        "id": "8lbiAjV3KW30"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_data = []\n",
        "for item in training_data:\n",
        "    contexts = \"\\n\".join([ctx['text'] for ctx in item['contexts']])\n",
        "    base_input = f\"Вопрос: {item['user_question']}\\nКонтекст: {contexts}\"\n",
        "\n",
        "    if item['winner'] == 'Saiga':\n",
        "        formatted_data.append({\n",
        "            \"input\": base_input,\n",
        "            \"output\": item['saiga_answer'],\n",
        "            \"source\": \"saiga\",\n",
        "            \"rating\": \"good\" if item['winner'] in ['Saiga', 'Оба хорошо'] else \"bad\"\n",
        "        })\n",
        "\n",
        "    elif item['winner'] == 'GigaChat':\n",
        "        formatted_data.append({\n",
        "            \"input\": base_input,\n",
        "            \"output\": item['giga_answer'],\n",
        "            \"source\": \"giga\",\n",
        "            \"rating\": \"good\" if item['winner'] in ['GigaChat', 'Оба хорошо'] else \"bad\"\n",
        "        })\n",
        "\n",
        "    elif item['winner'] == 'Оба хорошо':\n",
        "        formatted_data.extend([\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['saiga_answer'],\n",
        "                \"source\": \"saiga\",\n",
        "                \"rating\": \"good\"\n",
        "            },\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['giga_answer'],\n",
        "                \"source\": \"giga\",\n",
        "                \"rating\": \"good\"\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    elif item['winner'] == 'Оба плохо':\n",
        "        formatted_data.extend([\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['saiga_answer'],\n",
        "                \"source\": \"saiga\",\n",
        "                \"rating\": \"bad\"\n",
        "            },\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['giga_answer'],\n",
        "                \"source\": \"giga\",\n",
        "                \"rating\": \"bad\"\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    else:\n",
        "        formatted_data.append({\n",
        "            \"input\": base_input,\n",
        "            \"output\": item['saiga_answer'],\n",
        "            \"source\": \"unknown\",\n",
        "            \"rating\": \"neutral\"\n",
        "        })"
      ],
      "metadata": {
        "id": "Qnl3d60lKZfy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('parsed_dash.json', 'r', encoding='utf-8') as f:\n",
        "    val_data = json.load(f)"
      ],
      "metadata": {
        "id": "onQU9HsNKcID"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_data_val = []\n",
        "for item in val_data:\n",
        "    contexts = \"\\n\".join([ctx['text'] for ctx in item['contexts']])\n",
        "    base_input = f\"Вопрос: {item['user_question']}\\nКонтекст: {contexts}\"\n",
        "\n",
        "    if item['winner'] == 'Saiga':\n",
        "        formatted_data_val.append({\n",
        "            \"input\": base_input,\n",
        "            \"output\": item['saiga_answer'],\n",
        "            \"source\": \"saiga\",\n",
        "            \"rating\": \"good\" if item['winner'] in ['Saiga', 'Оба хорошо'] else \"bad\"\n",
        "        })\n",
        "\n",
        "    elif item['winner'] == 'GigaChat':\n",
        "        formatted_data_val.append({\n",
        "            \"input\": base_input,\n",
        "            \"output\": item['giga_answer'],\n",
        "            \"source\": \"giga\",\n",
        "            \"rating\": \"good\" if item['winner'] in ['GigaChat', 'Оба хорошо'] else \"bad\"\n",
        "        })\n",
        "\n",
        "    elif item['winner'] == 'Оба хорошо':\n",
        "        formatted_data_val.extend([\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['saiga_answer'],\n",
        "                \"source\": \"saiga\",\n",
        "                \"rating\": \"good\"\n",
        "            },\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['giga_answer'],\n",
        "                \"source\": \"giga\",\n",
        "                \"rating\": \"good\"\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    elif item['winner'] == 'Оба плохо':\n",
        "        formatted_data_val.extend([\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['saiga_answer'],\n",
        "                \"source\": \"saiga\",\n",
        "                \"rating\": \"bad\"\n",
        "            },\n",
        "            {\n",
        "                \"input\": base_input,\n",
        "                \"output\": item['giga_answer'],\n",
        "                \"source\": \"giga\",\n",
        "                \"rating\": \"bad\"\n",
        "            }\n",
        "        ])\n",
        "\n",
        "    else:\n",
        "        formatted_data_val.append({\n",
        "            \"input\": base_input,\n",
        "            \"output\": item['saiga_answer'],\n",
        "            \"source\": \"unknown\",\n",
        "            \"rating\": \"neutral\"\n",
        "        })"
      ],
      "metadata": {
        "id": "YaSKFX8SKfTT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"system_prompt.yaml\", \"r\") as f:\n",
        "  system_prompt = ''.join(f.readlines()).split('system_template:')[1].strip()\n",
        "\n",
        "\n",
        "\n",
        "formatted_data_prompt = []\n",
        "for item in formatted_data:\n",
        "    formatted_data_prompt.append({\n",
        "        \"input\": f\"{system_prompt}\\n\\n{item['input']}\",  # Добавляем системный промпт\n",
        "        \"output\": item[\"output\"]\n",
        "    })"
      ],
      "metadata": {
        "id": "oevTc85zth7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = [elem['input'] for elem in formatted_data if elem[\"rating\"] == \"good\"]\n",
        "train_labels = [elem['output'] for elem in formatted_data if elem[\"rating\"] == \"good\"]"
      ],
      "metadata": {
        "id": "zEsYfkhYKiCC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[0]"
      ],
      "metadata": {
        "id": "bFJBBBKOtSCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_inputs = [elem['input'] for elem in formatted_data_val]\n",
        "val_labels = [elem['output'] for elem in formatted_data_val]"
      ],
      "metadata": {
        "id": "FNOGDIs6KkqJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'yandex/YandexGPT-5-Lite-8B-pretrain'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.,\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "id": "77hvGPstkDJb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "9d58a3672ba444b1b37cd9fe707b60ff",
            "328fe25573b24e08aae81c706bb67393",
            "566bf2d333b34464998ef534dda4c628",
            "0f566dceeaf0476ba8eef17f37bf8043",
            "1595907d2bb14176be039c1ad4fdf67b",
            "322b3bd3490d43dc9399760fb447f8ef",
            "21337b60bf6a4221922a12750978047f",
            "132b7ff637974e909dd00654ef4af88c",
            "7a7e0c7684e84ed5aa8ce1f1e67ab898",
            "f9dae48cefca46c9a5960fb90be8f151",
            "65be71e1495b4b5c8bfa7fb23d2baa1e"
          ]
        },
        "outputId": "d1afa535-294b-40cd-8cd3-034509c56c70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d58a3672ba444b1b37cd9fe707b60ff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(inputs, labels, tokenizer, max_length=128):\n",
        "    input_encodings = tokenizer(\n",
        "        list(inputs), max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    label_encodings = tokenizer(\n",
        "        list(labels), max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return input_encodings, label_encodings\n",
        "\n",
        "train_inputs_enc, train_labels_enc = tokenize_data(train_inputs, train_labels, tokenizer)\n",
        "test_inputs_enc, test_labels_enc = tokenize_data(val_inputs, val_labels, tokenizer)"
      ],
      "metadata": {
        "id": "ImcBqpnSGwG1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
        "            \"labels\": self.labels[\"input_ids\"][idx],\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(train_inputs_enc, train_labels_enc)\n",
        "test_dataset = CustomDataset(test_inputs_enc, test_labels_enc)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=3)"
      ],
      "metadata": {
        "id": "ILYOnelMG3cl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader):\n",
        "  model.train()\n",
        "  epochs = 4\n",
        "  for epoch in range(epochs):\n",
        "      for batch in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          input_ids = batch[\"input_ids\"].to(device)\n",
        "          attention_mask = batch[\"attention_mask\"].to(device)\n",
        "          labels = batch[\"labels\"].to(device)\n",
        "\n",
        "          outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "          loss = outputs.loss\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      print(f\"Epoch {epoch + 1} Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "cZPHwOemHISN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Задача генерации текста\n",
        "    inference_mode=False,          # Режим вывода (False для обучения)\n",
        "    r=8,                           # Ранг адаптации\n",
        "    lora_alpha=16,                 # Масштабирующий коэффициент\n",
        "    lora_dropout=0.1,              # Dropout для LoRA\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']     # Целевые модули для адаптации\n",
        ")\n"
      ],
      "metadata": {
        "id": "bLvVNjWvIUv8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = get_peft_model(model, peft_config).to(device)\n",
        "lora_model.print_trainable_parameters()\n",
        "\"\"\"    \"togethercomputer/RedPajama-INCITE-3B-v1\",\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "p4d2tpF-IhkN",
        "outputId": "8dc65130-36ac-4888-8404-3c01cadda9d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6,815,744 || all params: 8,043,368,448 || trainable%: 0.0847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    \"togethercomputer/RedPajama-INCITE-3B-v1\",\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model, LoraConfig, TaskType\n",
        "init = {PromptTuningInit.TEXT: \"\"\"Ты - система информационного поиска Национального исследовательского университета «Высшая школа экономики» (НИУ ВШЭ). Тебе дан вопрос и релевантные отрывки текста из нескольких документов.\n",
        "Создай краткий и информативный ответ на заданный вопрос. Ты должна использовать только информацию из приведенных отрывков.\n",
        "Используй непредвзятый и журналистский тон. Не повторяй текст. Не пытайся придумать ответ.\n",
        "Если в контексте присутствуют ссылки на документы, выведи ответ со ссылками на источники.\n",
        "Создай окончательный ответ (\"FINAL ANSWER\").\n",
        "Отвечай только на русском языке за исключением специальных терминов.\n",
        "ПРИМЕР:\n",
        "QUESTION: Что такое ИУП?\n",
        "=========\n",
        "Content 1: Индивидуальный учебный план (ИУП) — документ студента, в соответствии с которым он обязуется освоить элементы обучения (дисциплин/элементов практической подготовки) в определенный период.\n",
        "По каждому элементу ИУПа хранится информация\n",
        "Content 2: период.\n",
        "По каждому элементу ИУПа хранится информация о технологиях обучения, сроках изучения и прохождения промежуточной аттестации, объеме учебной нагрузки и основании включения этого элемента в учебный план.\n",
        "Для студентов большинства\n",
        "Content 3: план.\n",
        "Для студентов большинства образовательных программ, ИУП составляется на полугодие, и все элементы, включенные в ИУП, обязательны для освоения в этот период.\n",
        "Как ознакомиться со своим ИУП\n",
        "Content 4: со своим ИУП. Ваш ИУП доступен в [электронной зачетке]( https://lms.hse.ru/index.php?page=gradebook) LMS НИУ ВШЭ. \\nПодробнее об ИУП: https://www.hse.ru/studyspravka/plan\n",
        "=========\n",
        "FINAL ANSWER: Индивидуальный учебный план (ИУП) — документ студента, в соответствии с которым он обязуется освоить элементы обучения (дисциплин/элементов практической подготовки) в определенный период. \\nИУП доступен в электронной зачетке LMS НИУ ВШЭ (https://lms.hse.ru/index.php?page=gradebook). \\nПодробнее об ИУП: https://www.hse.ru/studyspravka/plan\n",
        "\n",
        "Если вопрос остался непонятен или слишком широкий, веди диалог с пользователем и попроси уточнить необходимую информацию: \"Пожалуйста, уточните ...\". Проявляй эмпатию и поддержку.\n",
        "QUESTION: Я боюсь, что меня отчислят...\n",
        "=========\n",
        "Content 1: отчисления по данному основанию допускается, если у обучающегося отсутствует возможность продолжать обучение в университете в связи с возникшими обстоятельствами)\n",
        "Content 2: Для профессиональной помощи в Дирекции сопровождения отдельных категорий студентов работает психолог[Болдырева Наталья Анатольевна](https://www.hse.ru/org/persons/902647165) . Также в любое время можете получить консультацию специалистов[Центра психологического консультирования](https://www.hse.ru/cpc/), заполнив[онлайн-формудля записи на консультацию к психологу](https://bpm.hse.ru/Runtime/Runtime/Form/RPC__f__NewInsideRequest/?&usertype=1) на сайтеили позвонив потелефону 8(915) 260-06-20.\n",
        "Content 3: Студенты, КУД которых превышает критическое значение (с учетом отнесения студента к отдельной категории и наличия или отсутствия повтора критической ситуации), подлежат отчислению как не выполняющие обязанности по добросовестному освоению образовательной программы и выполнению учебного плана.\n",
        "=========\n",
        "FINAL ANSWER: Я понимаю ваши опасения. Пожалуйста, уточните в чем причина ваших опасений? Вас беспокоит успеваемость, проблемы с оплатой или другие вопросы? В НИУ ВШЭ также работают специалисты, которые могут оказать вам психологическую помощь. Могу поделиться необходимыми контактами Важно своевременно обращаться за помощью и поддержкой, чтобы избежать возможных проблем с обучением.\n",
        "\n",
        "Если в отрывках текста не содержится ответ на вопрос, то выведи релевантную информацию, которую ты смог найти. Избегай упоминания дат и иных цифр. Напиши: \"Я могу частично ответить на данный вопрос: ... Для получения более релевантной информации обратитесь в учебный офис своей программы.\"\n",
        "\n",
        "ПРИМЕР:\n",
        "QUESTION: Как получить стипендию Потанина?\n",
        "=========\n",
        "Content 1: отвечающие критериям, установленным в пункте 2.3 Положения, подают заявку в электронном видев срок до 15 декабря текущего календарного года путем заполнения электронной формы в личном кабинете аспиранта на корпоративном сайте (портале) НИУ ВШЭ (http://www.hse.ru/user). Для подачи заявки аспирант предварительно загружает публикации в личном кабинете на корпоративном сайте (портале) НИУ ВШЭ. Заявки на стипендию принимаются ежегодно с 15 ноября по 15 декабря.\n",
        "Content 2: Положение о назначении и выплате именной стипендии имени Е.Т. Гайдара аспирантам Аспирантской школы по экономике Национального исследовательского университета «Высшая школа экономики»\n",
        "=========\n",
        "FINAL ANSWER: Я нашла информацию о том, что на некоторые стипендии заявку можно подать в электронном виде через личный кабинет на корпоративном сайте НИУ ВШЭ. Однако, я не могу сориентировать о процессе подачи на стипендию Потанина. Для получения более релевантной информации обратитесь в учебный офис своей программы.\n",
        "\n",
        "Если вопрос пользователя не соответсвует тематике вопросов обучения НИУ ВШЭ, не этичен или содержит просьбы о помощи с домашним заданием, написанием ВКР и др, ответь следующим образом:\n",
        "\"Мне кажется неуместно здесь это обсуждать. Моя цель - проконсультировать тебя по вопросам обучения. Может, у тебя есть еще какие-то вопросы?\"\n",
        "\"\"\"}\n",
        "config_lora = lora_model.config\n",
        "\n",
        "\n",
        "pt_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=init,\n",
        "    num_virtual_tokens=13\n",
        ")\n",
        "\n",
        "pt_model = get_peft_model(lora_model, pt_config)\n",
        "pt_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XHBmjbSRY_r",
        "outputId": "4bd9dc8f-ff38-49e3-a3f5-136211376aea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 53,248 || all params: 8,043,421,696 || trainable%: 0.0007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(pt_model.parameters(), lr=5e-4)"
      ],
      "metadata": {
        "id": "vDC9X0jHJ-h2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(pt_model, optimizer, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG6VPJeLQdse",
        "outputId": "d492d059-2f22-425c-c2a0-f7fd178e058a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 7.359323024749756\n",
            "Epoch 2 Loss: 6.380817890167236\n",
            "Epoch 3 Loss: 7.699309349060059\n",
            "Epoch 4 Loss: 6.321041584014893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOP"
      ],
      "metadata": {
        "id": "DfN913fYqUuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, test_loader):\n",
        "  model.eval()\n",
        "  for batch in test_loader:\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device)\n",
        "\n",
        "      input_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
        "      true_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
        "\n",
        "      outputs = model.generate(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          max_length=50\n",
        "      )\n",
        "      predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "      for input_text, true_label, pred in zip(input_texts, true_labels, predictions):\n",
        "          print(\"-\" * 50)\n",
        "          print(f\"input_txt: {input_text}\")\n",
        "          print(f\"true_label: {true_label}\")\n",
        "          print(f\"true_pred: {pred}\")\n",
        "\n",
        "      break"
      ],
      "metadata": {
        "id": "GwnLjLKuHxbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from peft import PromptTuningConfig, get_peft_model\n",
        "\n",
        "# Загрузка токенизатора и модели GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "config = model.config\n",
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False, num_virtual_tokens=13,\n",
        "    num_layers=config.n_layer, token_dim=config.n_embd, num_attention_heads=config.n_head,\n",
        "    prompt_tuning_init=init\n",
        ")\n",
        "\n",
        "pt_model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Перевод модели в режим оценки\n",
        "pt_model.eval()\n",
        "\n",
        "def generate_response(input_text):\n",
        "    # Подготовка входного текста\n",
        "    input_with_prompt = peft_config.prompt + input_text\n",
        "    inputs = tokenizer.encode(input_with_prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Генерация ответа\n",
        "    with torch.no_grad():\n",
        "        outputs = pt_model.generate(inputs, max_length=100, num_return_sequences=1)\n",
        "\n",
        "    # Декодирование и возвращение ответа\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response[len(peft_config.prompt):]  # Убираем системный промпт\n",
        "\n",
        "# Пример использования\n",
        "user_input = \"Как мне справиться со стрессом на работе?\"\n",
        "response = generate_response(user_input)\n",
        "print(\"Ответ:\", response)\n"
      ],
      "metadata": {
        "id": "AK-794AWPG79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(pt_model, optimizer, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csfjq_7ZRd4-",
        "outputId": "8cfcc624-f338-4158-8b8d-1fafe5d81a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.05859375\n",
            "Epoch 2 Loss: 4.640625\n",
            "Epoch 3 Loss: 4.67578125\n",
            "Epoch 4 Loss: 4.62890625\n",
            "Epoch 5 Loss: 5.2421875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление переменных\n",
        "del model\n",
        "del lora_model\n",
        "del optimizer\n",
        "del train_loader\n",
        "# Освобождение памяти GPU (для PyTorch)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rHWlwr78MA7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(lora_model.base_model.model.lm_head.state_dict(), 'lora_head.pt')"
      ],
      "metadata": {
        "id": "xujZhqa2PwQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chkpt = torch.load('lora_head.pt')\n",
        "chkpt.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfJ1N2GaQ1kX",
        "outputId": "0f911beb-5612-4cf0-f750-3c2e55316bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-4c71755dc0ab>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  chkpt = torch.load('lora_head.pt')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('lora_head.pt')\n",
        "print(state_dict[''])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEBbjzCLaeWr",
        "outputId": "241a9981-f4d7-4a1d-bcd5-7696ee55cbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-fc7b80294d13>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('lora_head.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['weight'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.lm_head.load_state_dict({\n",
        "    'weight': torch.load('lora_head.pt')['weight']\n",
        "})\n",
        "optimizer = AdamW(model.parameters(), lr=2e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "7XhgQXLGQ5N_",
        "outputId": "25d17bd2-7079-4813-ffc1-ff92f7a406ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-9a7c70f25b48>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  'weight': torch.load('lora_head.pt')['weight']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 14.74 GiB of which 354.12 MiB is free. Process 5925 has 14.39 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 434.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9a7c70f25b48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.lm_head.load_state_dict({\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lora_head.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m })\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0m_internal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     ) -> Union[_StorageBase, TypedStorage]:\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             ), f\"sparse storage is not supported for {device.type.upper()} tensors\"\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0muntyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0muntyped_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0muntyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 14.74 GiB of which 354.12 MiB is free. Process 5925 has 14.39 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 434.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, optimizer, train_loader)"
      ],
      "metadata": {
        "id": "RggQa29pRBXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d58a3672ba444b1b37cd9fe707b60ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_328fe25573b24e08aae81c706bb67393",
              "IPY_MODEL_566bf2d333b34464998ef534dda4c628",
              "IPY_MODEL_0f566dceeaf0476ba8eef17f37bf8043"
            ],
            "layout": "IPY_MODEL_1595907d2bb14176be039c1ad4fdf67b"
          }
        },
        "328fe25573b24e08aae81c706bb67393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_322b3bd3490d43dc9399760fb447f8ef",
            "placeholder": "​",
            "style": "IPY_MODEL_21337b60bf6a4221922a12750978047f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "566bf2d333b34464998ef534dda4c628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_132b7ff637974e909dd00654ef4af88c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a7e0c7684e84ed5aa8ce1f1e67ab898",
            "value": 4
          }
        },
        "0f566dceeaf0476ba8eef17f37bf8043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9dae48cefca46c9a5960fb90be8f151",
            "placeholder": "​",
            "style": "IPY_MODEL_65be71e1495b4b5c8bfa7fb23d2baa1e",
            "value": " 4/4 [01:34&lt;00:00, 20.40s/it]"
          }
        },
        "1595907d2bb14176be039c1ad4fdf67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "322b3bd3490d43dc9399760fb447f8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21337b60bf6a4221922a12750978047f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "132b7ff637974e909dd00654ef4af88c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7e0c7684e84ed5aa8ce1f1e67ab898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9dae48cefca46c9a5960fb90be8f151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65be71e1495b4b5c8bfa7fb23d2baa1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}